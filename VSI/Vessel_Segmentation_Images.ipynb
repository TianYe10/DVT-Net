{"cells":[{"cell_type":"markdown","metadata":{"id":"8uvhQrsBMMcY"},"source":["***Vessel Segmentation (gray) for retina fundus images***\n","\n","Credit: SA - UNet (github) [SA - UNet](https://github.com/clguo/SA-UNet)\n","\n","This code requires an environment of tensorflow 2.0 version or above. The scipy==1.1.0 is recommended to avoid incompatible errors."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","\n","from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, \\\n","    Conv2D, Add, Activation, Lambda,Conv1D\n","from PIL import Image\n","import numpy as np\n","import glob\n","import os\n","import cv2\n","from keras.callbacks import TensorBoard, ModelCheckpoint\n","import matplotlib.pyplot as plt\n","from  scipy.misc.pilutil import *\n","from keras.optimizers import *\n","from keras.models import Model\n","from keras.layers import Input,Conv2DTranspose, MaxPooling2D,BatchNormalization,concatenate,Activation\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Data Preparation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdN6ERXpnMMi"},"outputs":[],"source":["data_path = '/content/drive/MyDrive/VascularSignaturesPreeclampsia/Datasets/DRIVE'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13682,"status":"ok","timestamp":1658176701084,"user":{"displayName":"Ye Tian","userId":"01053288204941555603"},"user_tz":240},"id":"cLlMnX8LmaTp","outputId":"a8050a60-bc70-48d3-d73e-81e91bb1c2b0"},"outputs":[],"source":["\n","image_number = []\n","image_list = []\n","for filename in glob.glob(data_path + 'training/images/*.tif'):\n","    num = filename.split('/')[-1].split('_')[0]\n","    im=Image.open(filename)\n","    image_number.append(num)\n","    #im.show()\n","    im = np.array(im)\n","    image_list.append(im)\n","\n","label_number = []\n","label_list = []\n","for filename in glob.glob(data_path + '/training/1st_manual/*.gif'):\n","    num = filename.split('/')[-1].split('_')[0]\n","    im=Image.open(filename)\n","    label_number.append(num)\n","    #im.show()\n","    im = np.array(im)\n","    label_list.append(im)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658177167435,"user":{"displayName":"Ye Tian","userId":"01053288204941555603"},"user_tz":240},"id":"ZZLnh5X1GgKe","outputId":"b7c11b17-90c3-4a91-e7ac-590f4ba209f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["total # of images: 68\n","total # of labels: 68\n"]}],"source":["x = image_list\n","y = label_list\n","\n","print('total # of images:', len(x))\n","print('total # of labels:', len(y))"]},{"cell_type":"markdown","metadata":{},"source":["Resize and max normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDqyLD_oHe1X"},"outputs":[],"source":["x_train = []\n","y_train = []\n","for i in range(len(x)):\n","    ratio_x = 1008 / x[i].shape[1]\n","    ratio_y = 1008 / x[i].shape[0]\n","    im_1008 = cv2.resize(x[i], (0, 0), fx=ratio_x, fy=ratio_y)\n","    x_train.append(im_1008)\n","    \n","    ratio_x = 1008 / y[i].shape[1]\n","    ratio_y = 1008 / y[i].shape[0]\n","    label_1008 = cv2.resize(y[i], (0, 0), fx=ratio_x, fy=ratio_y)\n","    y_train.append(label_1008)\n","\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","\n","x_train = x_train / 255\n","y_train = y_train / 255"]},{"cell_type":"markdown","metadata":{"id":"OwZuFqlkuBoo"},"source":["# **SA-UNet**\n","\n","### Drop Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJ1tebHkMQSs"},"outputs":[],"source":["\n","class DropBlock1D(keras.layers.Layer):\n","    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n","\n","    def __init__(self,\n","                 block_size,\n","                 keep_prob,\n","                 sync_channels=False,\n","                 data_format=None,\n","                 **kwargs):\n","        \"\"\"Initialize the layer.\n","        :param block_size: Size for each mask block.\n","        :param keep_prob: Probability of keeping the original feature.\n","        :param sync_channels: Whether to use the same dropout for all channels.\n","        :param data_format: 'channels_first' or 'channels_last' (default).\n","        :param kwargs: Arguments for parent class.\n","        \"\"\"\n","        super(DropBlock1D, self).__init__(**kwargs)\n","        self.block_size = block_size\n","        self.keep_prob = keep_prob\n","        self.sync_channels = sync_channels\n","        #self.data_format = K.normalize_data_format(data_format) keras.utils.conv_utils.normalize_data_format\n","        self.data_format = keras.utils.conv_utils.normalize_data_format(data_format) \n","        self.input_spec = keras.engine.base_layer.InputSpec(ndim=3)\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = {'block_size': self.block_size,\n","                  'keep_prob': self.keep_prob,\n","                  'sync_channels': self.sync_channels,\n","                  'data_format': self.data_format}\n","        base_config = super(DropBlock1D, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    def _get_gamma(self, feature_dim):\n","        \"\"\"Get the number of activation units to drop\"\"\"\n","        feature_dim = K.cast(feature_dim, K.floatx())\n","        block_size = K.constant(self.block_size, dtype=K.floatx())\n","        return ((1.0 - self.keep_prob) / block_size) * (feature_dim / (feature_dim - block_size + 1.0))\n","\n","    def _compute_valid_seed_region(self, seq_length):\n","        positions = K.arange(seq_length)\n","        half_block_size = self.block_size // 2\n","        valid_seed_region = K.switch(\n","            K.all(\n","                K.stack(\n","                    [\n","                        positions >= half_block_size,\n","                        positions < seq_length - half_block_size,\n","                    ],\n","                    axis=-1,\n","                ),\n","                axis=-1,\n","            ),\n","            K.ones((seq_length,)),\n","            K.zeros((seq_length,)),\n","        )\n","        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n","\n","    def _compute_drop_mask(self, shape):\n","        seq_length = shape[1]\n","        mask = K.random_binomial(shape, p=self._get_gamma(seq_length))\n","        mask *= self._compute_valid_seed_region(seq_length)\n","        mask = keras.layers.MaxPool1D(\n","            pool_size=self.block_size,\n","            padding='same',\n","            strides=1,\n","            data_format='channels_last',\n","        )(mask)\n","        return 1.0 - mask\n","\n","    def call(self, inputs):\n","\n","        def dropped_inputs():\n","            outputs = inputs\n","            if self.data_format == 'channels_first':\n","                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n","            shape = K.shape(outputs)\n","            if self.sync_channels:\n","                mask = self._compute_drop_mask([shape[0], shape[1], 1])\n","            else:\n","                mask = self._compute_drop_mask(shape)\n","            outputs = outputs * mask *\\\n","                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n","            if self.data_format == 'channels_first':\n","                outputs = K.permute_dimensions(outputs, [0, 2, 1])\n","            return outputs\n","\n","        return K.in_train_phase(dropped_inputs, inputs)\n","\n","\n","class DropBlock2D(keras.layers.Layer):\n","    \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n","\n","    def __init__(self,\n","                 block_size,\n","                 keep_prob,\n","                 sync_channels=False,\n","                 data_format=None,\n","                 **kwargs):\n","        \"\"\"Initialize the layer.\n","        :param block_size: Size for each mask block.\n","        :param keep_prob: Probability of keeping the original feature.\n","        :param sync_channels: Whether to use the same dropout for all channels.\n","        :param data_format: 'channels_first' or 'channels_last' (default).\n","        :param kwargs: Arguments for parent class.\n","        \"\"\"\n","        super(DropBlock2D, self).__init__(**kwargs)\n","        self.block_size = block_size\n","        self.keep_prob = keep_prob\n","        self.sync_channels = sync_channels\n","        self.data_format = keras.utils.conv_utils.normalize_data_format(data_format)\n","        self.input_spec = keras.engine.base_layer.InputSpec(ndim=4)\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = {'block_size': self.block_size,\n","                  'keep_prob': self.keep_prob,\n","                  'sync_channels': self.sync_channels,\n","                  'data_format': self.data_format}\n","        base_config = super(DropBlock2D, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    def _get_gamma(self, height, width):\n","        \"\"\"Get the number of activation units to drop\"\"\"\n","        height, width = K.cast(height, K.floatx()), K.cast(width, K.floatx())\n","        block_size = K.constant(self.block_size, dtype=K.floatx())\n","        return ((1.0 - self.keep_prob) / (block_size ** 2)) *\\\n","               (height * width / ((height - block_size + 1.0) * (width - block_size + 1.0)))\n","\n","    def _compute_valid_seed_region(self, height, width):\n","\n","        p1 = K.arange(height)\n","        p2 = K.arange(width)\n","\n","        t1 = K.expand_dims(p1, axis=1)\n","        t2 = K.expand_dims(p2, axis=0)\n","\n","        tp1 = K.tile(t1, [1, width])\n","        tp2 = K.tile(t2, [height, 1])\n","\n","        temp1 = K.expand_dims(tp1, axis=-1)\n","        temp2 = K.expand_dims(tp2, axis=-1)\n","                              \n","        positions = K.concatenate([temp1, temp2], axis=-1)\n","\n","\n","        half_block_size = self.block_size // 2\n","        valid_seed_region = K.switch(\n","            K.all(\n","                K.stack(\n","                    [\n","                        positions[:, :, 0] >= half_block_size,\n","                        positions[:, :, 1] >= half_block_size,\n","                        positions[:, :, 0] < height - half_block_size,\n","                        positions[:, :, 1] < width - half_block_size,\n","                    ],\n","                    axis=-1,\n","                ),\n","                axis=-1,\n","            ),\n","            K.ones((height, width)),\n","            K.zeros((height, width)),\n","        )\n","        return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n","\n","    def _compute_drop_mask(self, shape):\n","        height, width = shape[1], shape[2]\n","        mask = K.random_binomial(shape, p=self._get_gamma(height, width))\n","        mask *= self._compute_valid_seed_region(height, width)\n","        mask = keras.layers.MaxPool2D(\n","            pool_size=(self.block_size, self.block_size),\n","            padding='same',\n","            strides=1,\n","            data_format='channels_last',\n","        )(mask)\n","        return 1.0 - mask\n","\n","    def call(self, inputs, training = None):\n","\n","        def dropped_inputs():\n","            outputs = inputs\n","            if self.data_format == 'channels_first':\n","                outputs = K.permute_dimensions(outputs, [0, 2, 3, 1])\n","            shape = K.shape(outputs)\n","            if self.sync_channels:\n","                mask = self._compute_drop_mask([shape[0], shape[1], shape[2], 1])\n","            else:\n","                mask = self._compute_drop_mask(shape)\n","            outputs = outputs * mask *\\\n","                (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n","            if self.data_format == 'channels_first':\n","                outputs = K.permute_dimensions(outputs, [0, 3, 1, 2])\n","            return outputs\n","\n","        return K.in_train_phase(dropped_inputs, inputs, training=training)"]},{"cell_type":"markdown","metadata":{"id":"Wb4FALYlMWiR"},"source":["## Spatial Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dfk23mpqMaW-"},"outputs":[],"source":["\n","def spatial_attention(input_feature):\n","    kernel_size = 7\n","\n","    if K.image_data_format() == \"channels_first\":\n","        channel = input_feature.shape[1]\n","        cbam_feature = Permute((2, 3, 1))(input_feature)\n","    else:\n","        channel = input_feature.shape[-1]\n","        cbam_feature = input_feature\n","\n","    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n","    assert avg_pool.shape[-1] == 1\n","    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n","    assert max_pool.shape[-1] == 1\n","    concat = Concatenate(axis=3)([avg_pool, max_pool])\n","    assert concat.shape[-1] == 2\n","    cbam_feature = Conv2D(filters=1,\n","                          kernel_size=kernel_size,\n","                          strides=1,\n","                          padding='same',\n","                          activation='sigmoid',\n","                          kernel_initializer='he_normal',\n","                          use_bias=False)(concat)\n","    assert cbam_feature.shape[-1] == 1\n","\n","    if K.image_data_format() == \"channels_first\":\n","        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n","\n","    return multiply([input_feature, cbam_feature])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Backbone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWRt3E7gMkBB"},"outputs":[],"source":["\n","\n","def Backbone(input_size=(1440, 1440, 3), block_size=7,keep_prob=0.9,start_neurons=16,lr=1e-3):\n","\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(inputs)\n","    conv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv1)\n","    conv1= BatchNormalization()(conv1)\n","    conv1 = Activation('relu')(conv1)\n","    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(conv1)\n","    conv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv1)\n","    conv1 = BatchNormalization()(conv1)\n","    conv1 = Activation('relu')(conv1)\n","    pool1 = MaxPooling2D((2, 2))(conv1)\n","\n","\n","\n","    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n","    conv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv2)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","\n","    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(conv2)\n","    conv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv2)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","    pool2 = MaxPooling2D((2, 2))(conv2)\n","\n","\n","    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n","    conv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(conv3)\n","    conv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    pool3 = MaxPooling2D((2, 2))(conv3)\n","\n","\n","    convm = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n","    convm = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(convm)\n","    convm = BatchNormalization()(convm)\n","    convm = Activation('relu')(convm)\n","    convm = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(convm)\n","    convm = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(convm)\n","    convm = BatchNormalization()(convm)\n","    convm = Activation('relu')(convm)\n","\n","\n","    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n","    uconv3 = concatenate([deconv3, conv3])\n","\n","    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n","    uconv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv3)\n","    uconv3 = BatchNormalization()(uconv3)\n","    uconv3 = Activation('relu')(uconv3)\n","    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n","    uconv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv3)\n","    uconv3 = BatchNormalization()(uconv3)\n","    uconv3 = Activation('relu')(uconv3)\n","\n","    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n","    uconv2 = concatenate([deconv2, conv2])\n","\n","    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n","    uconv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv2)\n","    uconv2 = BatchNormalization()(uconv2)\n","    uconv2 = Activation('relu')(uconv2)\n","    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n","    uconv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv2)\n","    uconv2 = BatchNormalization()(uconv2)\n","    uconv2 = Activation('relu')(uconv2)\n","\n","    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n","    uconv1 = concatenate([deconv1, conv1])\n","\n","\n","    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n","    uconv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv1)\n","    uconv1 = BatchNormalization()(uconv1)\n","    uconv1 = Activation('relu')(uconv1)\n","    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n","    uconv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv1)\n","    uconv1 = BatchNormalization()(uconv1)\n","    uconv1 = Activation('relu')(uconv1)\n","    output_layer_noActi = Conv2D(1, (1, 1), padding=\"same\", activation=None)(uconv1)\n","    output_layer = Activation('sigmoid')(output_layer_noActi)\n","\n","    model = Model(input=inputs, output=output_layer)\n","\n","    model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","def SA_UNet(input_size=(512, 512, 3), block_size=7,keep_prob=0.9,start_neurons=16,lr=1e-3):\n","\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(inputs)\n","    #conv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv1)\n","    conv1= BatchNormalization()(conv1)\n","    conv1 = Activation('relu')(conv1)\n","    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(conv1)\n","    #conv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv1)\n","    conv1 = BatchNormalization()(conv1)\n","    conv1 = Activation('relu')(conv1)\n","    pool1 = MaxPooling2D((2, 2))(conv1)\n","\n","\n","\n","    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n","    #conv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv2)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","\n","    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(conv2)\n","    #conv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv2)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","    pool2 = MaxPooling2D((2, 2))(conv2)\n","\n","\n","    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n","    #conv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(conv3)\n","    #conv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    pool3 = MaxPooling2D((2, 2))(conv3)\n","\n","\n","    convm = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n","    #convm = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(convm)\n","    convm = BatchNormalization()(convm)\n","    convm = Activation('relu')(convm)\n","    convm = spatial_attention(convm)\n","    convm = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(convm)\n","    #convm = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(convm)\n","    convm = BatchNormalization()(convm)\n","    convm = Activation('relu')(convm)\n","\n","\n","    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n","    uconv3 = concatenate([deconv3, conv3])\n","\n","    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n","    #uconv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv3)\n","    uconv3 = BatchNormalization()(uconv3)\n","    uconv3 = Activation('relu')(uconv3)\n","    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n","    #uconv3 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv3)\n","    uconv3 = BatchNormalization()(uconv3)\n","    uconv3 = Activation('relu')(uconv3)\n","\n","    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n","    uconv2 = concatenate([deconv2, conv2])\n","\n","    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n","    #uconv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv2)\n","    uconv2 = BatchNormalization()(uconv2)\n","    uconv2 = Activation('relu')(uconv2)\n","    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n","    #uconv2 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv2)\n","    uconv2 = BatchNormalization()(uconv2)\n","    uconv2 = Activation('relu')(uconv2)\n","\n","    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n","    uconv1 = concatenate([deconv1, conv1])\n","\n","\n","    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n","    #uconv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv1)\n","    uconv1 = BatchNormalization()(uconv1)\n","    uconv1 = Activation('relu')(uconv1)\n","    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n","    #uconv1 = DropBlock2D(block_size=block_size, keep_prob=keep_prob)(uconv1)\n","    uconv1 = BatchNormalization()(uconv1)\n","    uconv1 = Activation('relu')(uconv1)\n","    output_layer_noActi = Conv2D(1, (1, 1), padding=\"same\", activation=None)(uconv1)\n","    output_layer = Activation('sigmoid')(output_layer_noActi)\n","\n","    model = Model(inputs=inputs, outputs=output_layer)\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["Train from Scratch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_location = '.../'\n","\n","training_images_loc = data_location + 'training/training/images/'\n","training_label_loc = data_location + 'training/training/1st_manual/'\n","\n","validate_images_loc = data_location + 'validation/validation/images/'\n","validate_label_loc = data_location + 'validation/validation/1st_manual/'\n","train_files = os.listdir(training_images_loc)\n","train_data = []\n","train_label = []\n","validate_files = os.listdir(validate_images_loc)\n","validate_data = []\n","validate_label = []\n","desired_size = 592\n","for i in train_files:\n","    im = imread(training_images_loc + i)\n","    label = imread(training_label_loc + i.split('_')[0] + '_manual1.gif',mode=\"L\")\n","    old_size = im.shape[:2]  # old_size is in (height, width) format\n","    delta_w = desired_size - old_size[1]\n","    delta_h = desired_size - old_size[0]\n","\n","    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n","    left, right = delta_w // 2, delta_w - (delta_w // 2)\n","\n","    color = [0, 0, 0]\n","    color2 = [0]\n","    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n","                                value=color)\n","\n","    new_label = cv2.copyMakeBorder(label, top, bottom, left, right, cv2.BORDER_CONSTANT,\n","                                   value=color2)\n","\n","    train_data.append(cv2.resize(new_im, (desired_size, desired_size)))\n","\n","    temp = cv2.resize(new_label, (desired_size, desired_size))\n","    _, temp = cv2.threshold(temp, 127, 255, cv2.THRESH_BINARY)\n","    train_label.append(temp)\n","\n","for i in validate_files:\n","    im = imread(validate_images_loc + i)\n","    label = imread(validate_label_loc + i.split('_')[0] + '_manual1.gif',mode=\"L\")\n","    old_size = im.shape[:2]  # old_size is in (height, width) format\n","    delta_w = desired_size - old_size[1]\n","    delta_h = desired_size - old_size[0]\n","\n","    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n","    left, right = delta_w // 2, delta_w - (delta_w // 2)\n","\n","    color = [0, 0, 0]\n","    color2 = [0]\n","    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n","                                value=color)\n","\n","    new_label = cv2.copyMakeBorder(label, top, bottom, left, right, cv2.BORDER_CONSTANT,\n","                                   value=color2)\n","\n","    validate_data.append(cv2.resize(new_im, (desired_size, desired_size)))\n","\n","    temp = cv2.resize(new_label, (desired_size, desired_size))\n","    _, temp = cv2.threshold(temp, 127, 255, cv2.THRESH_BINARY)\n","    validate_label.append(temp)\n","\n","train_data = np.array(train_data)\n","train_label = np.array(train_label)\n","\n","validate_data = np.array(validate_data)\n","validate_label = np.array(validate_label)\n","\n","x_train = train_data.astype('float32') / 255.\n","y_train = train_label.astype('float32') / 255.\n","x_train = np.reshape(x_train, (\n","len(x_train), desired_size, desired_size, 3))  # adapt this if using `channels_first` image data format\n","y_train = np.reshape(y_train, (len(y_train), desired_size, desired_size, 1))  # adapt this if using `channels_first` im\n","\n","x_validate = validate_data.astype('float32') / 255.\n","y_validate = validate_label.astype('float32') / 255.\n","x_validate = np.reshape(x_validate, (\n","len(x_validate), desired_size, desired_size, 3))  # adapt this if using `channels_first` image data format\n","y_validate = np.reshape(y_validate,\n","                        (len(y_validate), desired_size, desired_size, 1))  # adapt this if using `channels_first` im\n","\n","TensorBoard(log_dir='./autoencoder', histogram_freq=0,\n","            write_graph=True, write_images=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["Fine-tune pretrained models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":347260,"status":"ok","timestamp":1658356940986,"user":{"displayName":"Ye Tian","userId":"01053288204941555603"},"user_tz":240},"id":"HG12QMD8vHim","outputId":"ccbb1377-7678-448f-c930-03db304c4cb3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning:     `imread` is deprecated!\n","    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","    Use ``imageio.imread`` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning:     `imread` is deprecated!\n","    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","    Use ``imageio.imread`` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: DeprecationWarning:     `imread` is deprecated!\n","    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","    Use ``imageio.imread`` instead.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: DeprecationWarning:     `imread` is deprecated!\n","    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n","    Use ``imageio.imread`` instead.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model_12\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_13 (InputLayer)          [(None, 592, 592, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," conv2d_192 (Conv2D)            (None, 592, 592, 16  448         ['input_13[0][0]']               \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_168 (Batch  (None, 592, 592, 16  64         ['conv2d_192[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_180 (Activation)    (None, 592, 592, 16  0           ['batch_normalization_168[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_193 (Conv2D)            (None, 592, 592, 16  2320        ['activation_180[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_169 (Batch  (None, 592, 592, 16  64         ['conv2d_193[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_181 (Activation)    (None, 592, 592, 16  0           ['batch_normalization_169[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_36 (MaxPooling2D  (None, 296, 296, 16  0          ['activation_181[0][0]']         \n"," )                              )                                                                 \n","                                                                                                  \n"," conv2d_194 (Conv2D)            (None, 296, 296, 32  4640        ['max_pooling2d_36[0][0]']       \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_170 (Batch  (None, 296, 296, 32  128        ['conv2d_194[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_182 (Activation)    (None, 296, 296, 32  0           ['batch_normalization_170[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_195 (Conv2D)            (None, 296, 296, 32  9248        ['activation_182[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_171 (Batch  (None, 296, 296, 32  128        ['conv2d_195[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_183 (Activation)    (None, 296, 296, 32  0           ['batch_normalization_171[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_37 (MaxPooling2D  (None, 148, 148, 32  0          ['activation_183[0][0]']         \n"," )                              )                                                                 \n","                                                                                                  \n"," conv2d_196 (Conv2D)            (None, 148, 148, 64  18496       ['max_pooling2d_37[0][0]']       \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_172 (Batch  (None, 148, 148, 64  256        ['conv2d_196[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_184 (Activation)    (None, 148, 148, 64  0           ['batch_normalization_172[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_197 (Conv2D)            (None, 148, 148, 64  36928       ['activation_184[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_173 (Batch  (None, 148, 148, 64  256        ['conv2d_197[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_185 (Activation)    (None, 148, 148, 64  0           ['batch_normalization_173[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," max_pooling2d_38 (MaxPooling2D  (None, 74, 74, 64)  0           ['activation_185[0][0]']         \n"," )                                                                                                \n","                                                                                                  \n"," conv2d_198 (Conv2D)            (None, 74, 74, 128)  73856       ['max_pooling2d_38[0][0]']       \n","                                                                                                  \n"," batch_normalization_174 (Batch  (None, 74, 74, 128)  512        ['conv2d_198[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," activation_186 (Activation)    (None, 74, 74, 128)  0           ['batch_normalization_174[0][0]']\n","                                                                                                  \n"," lambda_24 (Lambda)             (None, 74, 74, 1)    0           ['activation_186[0][0]']         \n","                                                                                                  \n"," lambda_25 (Lambda)             (None, 74, 74, 1)    0           ['activation_186[0][0]']         \n","                                                                                                  \n"," concatenate_48 (Concatenate)   (None, 74, 74, 2)    0           ['lambda_24[0][0]',              \n","                                                                  'lambda_25[0][0]']              \n","                                                                                                  \n"," conv2d_199 (Conv2D)            (None, 74, 74, 1)    98          ['concatenate_48[0][0]']         \n","                                                                                                  \n"," multiply_12 (Multiply)         (None, 74, 74, 128)  0           ['activation_186[0][0]',         \n","                                                                  'conv2d_199[0][0]']             \n","                                                                                                  \n"," conv2d_200 (Conv2D)            (None, 74, 74, 128)  147584      ['multiply_12[0][0]']            \n","                                                                                                  \n"," batch_normalization_175 (Batch  (None, 74, 74, 128)  512        ['conv2d_200[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," activation_187 (Activation)    (None, 74, 74, 128)  0           ['batch_normalization_175[0][0]']\n","                                                                                                  \n"," conv2d_transpose_36 (Conv2DTra  (None, 148, 148, 64  73792      ['activation_187[0][0]']         \n"," nspose)                        )                                                                 \n","                                                                                                  \n"," concatenate_49 (Concatenate)   (None, 148, 148, 12  0           ['conv2d_transpose_36[0][0]',    \n","                                8)                                'activation_185[0][0]']         \n","                                                                                                  \n"," conv2d_201 (Conv2D)            (None, 148, 148, 64  73792       ['concatenate_49[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_176 (Batch  (None, 148, 148, 64  256        ['conv2d_201[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_188 (Activation)    (None, 148, 148, 64  0           ['batch_normalization_176[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":[" conv2d_202 (Conv2D)            (None, 148, 148, 64  36928       ['activation_188[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_177 (Batch  (None, 148, 148, 64  256        ['conv2d_202[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_189 (Activation)    (None, 148, 148, 64  0           ['batch_normalization_177[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_transpose_37 (Conv2DTra  (None, 296, 296, 32  18464      ['activation_189[0][0]']         \n"," nspose)                        )                                                                 \n","                                                                                                  \n"," concatenate_50 (Concatenate)   (None, 296, 296, 64  0           ['conv2d_transpose_37[0][0]',    \n","                                )                                 'activation_183[0][0]']         \n","                                                                                                  \n"," conv2d_203 (Conv2D)            (None, 296, 296, 32  18464       ['concatenate_50[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_178 (Batch  (None, 296, 296, 32  128        ['conv2d_203[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_190 (Activation)    (None, 296, 296, 32  0           ['batch_normalization_178[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_204 (Conv2D)            (None, 296, 296, 32  9248        ['activation_190[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_179 (Batch  (None, 296, 296, 32  128        ['conv2d_204[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_191 (Activation)    (None, 296, 296, 32  0           ['batch_normalization_179[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_transpose_38 (Conv2DTra  (None, 592, 592, 16  4624       ['activation_191[0][0]']         \n"," nspose)                        )                                                                 \n","                                                                                                  \n"," concatenate_51 (Concatenate)   (None, 592, 592, 32  0           ['conv2d_transpose_38[0][0]',    \n","                                )                                 'activation_181[0][0]']         \n","                                                                                                  \n"," conv2d_205 (Conv2D)            (None, 592, 592, 16  4624        ['concatenate_51[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_180 (Batch  (None, 592, 592, 16  64         ['conv2d_205[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_192 (Activation)    (None, 592, 592, 16  0           ['batch_normalization_180[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_206 (Conv2D)            (None, 592, 592, 16  2320        ['activation_192[0][0]']         \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_181 (Batch  (None, 592, 592, 16  64         ['conv2d_206[0][0]']             \n"," Normalization)                 )                                                                 \n","                                                                                                  \n"," activation_193 (Activation)    (None, 592, 592, 16  0           ['batch_normalization_181[0][0]']\n","                                )                                                                 \n","                                                                                                  \n"," conv2d_207 (Conv2D)            (None, 592, 592, 1)  17          ['activation_193[0][0]']         \n","                                                                                                  \n"," activation_194 (Activation)    (None, 592, 592, 1)  0           ['conv2d_207[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 538,707\n","Trainable params: 537,299\n","Non-trainable params: 1,408\n","__________________________________________________________________________________________________\n","Epoch 1/300\n","4/4 [==============================] - ETA: 0s - loss: 0.8273 - accuracy: 0.3804\n","Epoch 1: val_accuracy improved from -inf to 0.48999, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 4s 693ms/step - loss: 0.8273 - accuracy: 0.3804 - val_loss: 0.6933 - val_accuracy: 0.4900\n","Epoch 2/300\n","4/4 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.7561\n","Epoch 2: val_accuracy improved from 0.48999 to 0.91498, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 349ms/step - loss: 0.6520 - accuracy: 0.7561 - val_loss: 0.6820 - val_accuracy: 0.9150\n","Epoch 3/300\n","4/4 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.8850\n","Epoch 3: val_accuracy improved from 0.91498 to 0.91797, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 375ms/step - loss: 0.5629 - accuracy: 0.8850 - val_loss: 0.6645 - val_accuracy: 0.9180\n","Epoch 4/300\n","4/4 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.9011\n","Epoch 4: val_accuracy improved from 0.91797 to 0.91834, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 353ms/step - loss: 0.5095 - accuracy: 0.9011 - val_loss: 0.6473 - val_accuracy: 0.9183\n","Epoch 5/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.9061\n","Epoch 5: val_accuracy improved from 0.91834 to 0.91836, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 377ms/step - loss: 0.4801 - accuracy: 0.9061 - val_loss: 0.6327 - val_accuracy: 0.9184\n","Epoch 6/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4615 - accuracy: 0.9095\n","Epoch 6: val_accuracy improved from 0.91836 to 0.91838, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.4615 - accuracy: 0.9095 - val_loss: 0.6054 - val_accuracy: 0.9184\n","Epoch 7/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4496 - accuracy: 0.9124\n","Epoch 7: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 291ms/step - loss: 0.4496 - accuracy: 0.9124 - val_loss: 0.5848 - val_accuracy: 0.9184\n","Epoch 8/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.9129\n","Epoch 8: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.4391 - accuracy: 0.9129 - val_loss: 0.5652 - val_accuracy: 0.9184\n","Epoch 9/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.9141\n","Epoch 9: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.4261 - accuracy: 0.9141 - val_loss: 0.5516 - val_accuracy: 0.9184\n","Epoch 10/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.9157\n","Epoch 10: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.4139 - accuracy: 0.9157 - val_loss: 0.5390 - val_accuracy: 0.9184\n","Epoch 11/300\n","4/4 [==============================] - ETA: 0s - loss: 0.4043 - accuracy: 0.9171\n","Epoch 11: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 290ms/step - loss: 0.4043 - accuracy: 0.9171 - val_loss: 0.5257 - val_accuracy: 0.9184\n","Epoch 12/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.9178\n","Epoch 12: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.3958 - accuracy: 0.9178 - val_loss: 0.5127 - val_accuracy: 0.9184\n","Epoch 13/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.9184\n","Epoch 13: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.3872 - accuracy: 0.9184 - val_loss: 0.4994 - val_accuracy: 0.9184\n","Epoch 14/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.9189\n","Epoch 14: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 270ms/step - loss: 0.3803 - accuracy: 0.9189 - val_loss: 0.4835 - val_accuracy: 0.9184\n","Epoch 15/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.9202\n","Epoch 15: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 307ms/step - loss: 0.3712 - accuracy: 0.9202 - val_loss: 0.4652 - val_accuracy: 0.9184\n","Epoch 16/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.9150\n","Epoch 16: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 307ms/step - loss: 0.3844 - accuracy: 0.9150 - val_loss: 0.4492 - val_accuracy: 0.9184\n","Epoch 17/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.9237\n","Epoch 17: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 314ms/step - loss: 0.3622 - accuracy: 0.9237 - val_loss: 0.4354 - val_accuracy: 0.9184\n","Epoch 18/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3522 - accuracy: 0.9248\n","Epoch 18: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 319ms/step - loss: 0.3522 - accuracy: 0.9248 - val_loss: 0.4246 - val_accuracy: 0.9184\n","Epoch 19/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.9369\n","Epoch 19: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.3395 - accuracy: 0.9369 - val_loss: 0.4080 - val_accuracy: 0.9184\n","Epoch 20/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.9353\n","Epoch 20: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 294ms/step - loss: 0.3342 - accuracy: 0.9353 - val_loss: 0.4112 - val_accuracy: 0.9184\n","Epoch 21/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.9381\n","Epoch 21: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.3249 - accuracy: 0.9381 - val_loss: 0.4110 - val_accuracy: 0.9184\n","Epoch 22/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3135 - accuracy: 0.9436\n","Epoch 22: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 296ms/step - loss: 0.3135 - accuracy: 0.9436 - val_loss: 0.3980 - val_accuracy: 0.9184\n","Epoch 23/300\n","4/4 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9446\n","Epoch 23: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.3074 - accuracy: 0.9446 - val_loss: 0.3836 - val_accuracy: 0.9184\n","Epoch 24/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9475\n","Epoch 24: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 274ms/step - loss: 0.2978 - accuracy: 0.9475 - val_loss: 0.3760 - val_accuracy: 0.9184\n","Epoch 25/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9457\n","Epoch 25: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 298ms/step - loss: 0.2908 - accuracy: 0.9457 - val_loss: 0.3776 - val_accuracy: 0.9184\n","Epoch 26/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9479\n","Epoch 26: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 272ms/step - loss: 0.2825 - accuracy: 0.9479 - val_loss: 0.3664 - val_accuracy: 0.9184\n","Epoch 27/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9513\n","Epoch 27: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 295ms/step - loss: 0.2758 - accuracy: 0.9513 - val_loss: 0.3699 - val_accuracy: 0.9184\n","Epoch 28/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9493\n","Epoch 28: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 294ms/step - loss: 0.2716 - accuracy: 0.9493 - val_loss: 0.3545 - val_accuracy: 0.9184\n","Epoch 29/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9478\n","Epoch 29: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.2653 - accuracy: 0.9478 - val_loss: 0.3607 - val_accuracy: 0.9184\n","Epoch 30/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9522\n","Epoch 30: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 273ms/step - loss: 0.2573 - accuracy: 0.9522 - val_loss: 0.3554 - val_accuracy: 0.9184\n","Epoch 31/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9528\n","Epoch 31: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 269ms/step - loss: 0.2494 - accuracy: 0.9528 - val_loss: 0.3541 - val_accuracy: 0.9184\n","Epoch 32/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9546\n","Epoch 32: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 270ms/step - loss: 0.2433 - accuracy: 0.9546 - val_loss: 0.3524 - val_accuracy: 0.9184\n","Epoch 33/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9549\n","Epoch 33: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.2378 - accuracy: 0.9549 - val_loss: 0.3532 - val_accuracy: 0.9184\n","Epoch 34/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9530\n","Epoch 34: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 294ms/step - loss: 0.2363 - accuracy: 0.9530 - val_loss: 0.3505 - val_accuracy: 0.9184\n","Epoch 35/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9569\n","Epoch 35: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.2285 - accuracy: 0.9569 - val_loss: 0.3572 - val_accuracy: 0.9184\n","Epoch 36/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9561\n","Epoch 36: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.2250 - accuracy: 0.9561 - val_loss: 0.3533 - val_accuracy: 0.9184\n","Epoch 37/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9565\n","Epoch 37: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 293ms/step - loss: 0.2204 - accuracy: 0.9565 - val_loss: 0.3521 - val_accuracy: 0.9184\n","Epoch 38/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.9564\n","Epoch 38: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 294ms/step - loss: 0.2166 - accuracy: 0.9564 - val_loss: 0.3477 - val_accuracy: 0.9184\n","Epoch 39/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9582\n","Epoch 39: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 271ms/step - loss: 0.2121 - accuracy: 0.9582 - val_loss: 0.3529 - val_accuracy: 0.9181\n","Epoch 40/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9572\n","Epoch 40: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 292ms/step - loss: 0.2094 - accuracy: 0.9572 - val_loss: 0.3478 - val_accuracy: 0.9183\n","Epoch 41/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9584\n","Epoch 41: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.2057 - accuracy: 0.9584 - val_loss: 0.3491 - val_accuracy: 0.9175\n","Epoch 42/300\n","4/4 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9580\n","Epoch 42: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 291ms/step - loss: 0.2019 - accuracy: 0.9580 - val_loss: 0.3469 - val_accuracy: 0.9178\n","Epoch 43/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9586\n","Epoch 43: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.1984 - accuracy: 0.9586 - val_loss: 0.3473 - val_accuracy: 0.9171\n","Epoch 44/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9598\n","Epoch 44: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1938 - accuracy: 0.9598 - val_loss: 0.3447 - val_accuracy: 0.9166\n","Epoch 45/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9600\n","Epoch 45: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.1906 - accuracy: 0.9600 - val_loss: 0.3422 - val_accuracy: 0.9160\n","Epoch 46/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9594\n","Epoch 46: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.1888 - accuracy: 0.9594 - val_loss: 0.3391 - val_accuracy: 0.9164\n","Epoch 47/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9607\n","Epoch 47: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1853 - accuracy: 0.9607 - val_loss: 0.3449 - val_accuracy: 0.9147\n","Epoch 48/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9595\n","Epoch 48: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1846 - accuracy: 0.9595 - val_loss: 0.3400 - val_accuracy: 0.9157\n","Epoch 49/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9611\n","Epoch 49: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1797 - accuracy: 0.9611 - val_loss: 0.3412 - val_accuracy: 0.9136\n","Epoch 50/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9608\n","Epoch 50: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1787 - accuracy: 0.9608 - val_loss: 0.3412 - val_accuracy: 0.9141\n","Epoch 51/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9602\n","Epoch 51: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1761 - accuracy: 0.9602 - val_loss: 0.3468 - val_accuracy: 0.9133\n","Epoch 52/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9606\n","Epoch 52: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1737 - accuracy: 0.9606 - val_loss: 0.3468 - val_accuracy: 0.9136\n","Epoch 53/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9615\n","Epoch 53: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 290ms/step - loss: 0.1702 - accuracy: 0.9615 - val_loss: 0.3478 - val_accuracy: 0.9128\n","Epoch 54/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9616\n","Epoch 54: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 264ms/step - loss: 0.1677 - accuracy: 0.9616 - val_loss: 0.3446 - val_accuracy: 0.9134\n","Epoch 55/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9619\n","Epoch 55: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 294ms/step - loss: 0.1653 - accuracy: 0.9619 - val_loss: 0.3484 - val_accuracy: 0.9123\n","Epoch 56/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9627\n","Epoch 56: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 264ms/step - loss: 0.1629 - accuracy: 0.9627 - val_loss: 0.3554 - val_accuracy: 0.9106\n","Epoch 57/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.9619\n","Epoch 57: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 288ms/step - loss: 0.1612 - accuracy: 0.9619 - val_loss: 0.3598 - val_accuracy: 0.9104\n","Epoch 58/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9634\n","Epoch 58: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1586 - accuracy: 0.9634 - val_loss: 0.3598 - val_accuracy: 0.9097\n","Epoch 59/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1569 - accuracy: 0.9625\n","Epoch 59: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 264ms/step - loss: 0.1569 - accuracy: 0.9625 - val_loss: 0.3619 - val_accuracy: 0.9099\n","Epoch 60/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9628\n","Epoch 60: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 290ms/step - loss: 0.1561 - accuracy: 0.9628 - val_loss: 0.3673 - val_accuracy: 0.9091\n","Epoch 61/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1545 - accuracy: 0.9621\n","Epoch 61: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1545 - accuracy: 0.9621 - val_loss: 0.3696 - val_accuracy: 0.9083\n","Epoch 62/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9636\n","Epoch 62: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 291ms/step - loss: 0.1518 - accuracy: 0.9636 - val_loss: 0.3729 - val_accuracy: 0.9079\n","Epoch 63/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9633\n","Epoch 63: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 268ms/step - loss: 0.1489 - accuracy: 0.9633 - val_loss: 0.3757 - val_accuracy: 0.9072\n","Epoch 64/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9635\n","Epoch 64: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1481 - accuracy: 0.9635 - val_loss: 0.3746 - val_accuracy: 0.9082\n","Epoch 65/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9638\n","Epoch 65: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1464 - accuracy: 0.9638 - val_loss: 0.3726 - val_accuracy: 0.9076\n","Epoch 66/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9638\n","Epoch 66: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1444 - accuracy: 0.9638 - val_loss: 0.3709 - val_accuracy: 0.9082\n","Epoch 67/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9640\n","Epoch 67: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 293ms/step - loss: 0.1431 - accuracy: 0.9640 - val_loss: 0.3699 - val_accuracy: 0.9099\n","Epoch 68/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9643\n","Epoch 68: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 290ms/step - loss: 0.1412 - accuracy: 0.9643 - val_loss: 0.3690 - val_accuracy: 0.9100\n","Epoch 69/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9648\n","Epoch 69: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 264ms/step - loss: 0.1392 - accuracy: 0.9648 - val_loss: 0.3672 - val_accuracy: 0.9096\n","Epoch 70/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9640\n","Epoch 70: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 292ms/step - loss: 0.1383 - accuracy: 0.9640 - val_loss: 0.3684 - val_accuracy: 0.9095\n","Epoch 71/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9650\n","Epoch 71: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1368 - accuracy: 0.9650 - val_loss: 0.3659 - val_accuracy: 0.9098\n","Epoch 72/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9649\n","Epoch 72: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 291ms/step - loss: 0.1347 - accuracy: 0.9649 - val_loss: 0.3666 - val_accuracy: 0.9107\n","Epoch 73/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9656\n","Epoch 73: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 292ms/step - loss: 0.1330 - accuracy: 0.9656 - val_loss: 0.3627 - val_accuracy: 0.9114\n","Epoch 74/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.9650\n","Epoch 74: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1322 - accuracy: 0.9650 - val_loss: 0.3563 - val_accuracy: 0.9125\n","Epoch 75/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9654\n","Epoch 75: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1315 - accuracy: 0.9654 - val_loss: 0.3496 - val_accuracy: 0.9132\n","Epoch 76/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9654\n","Epoch 76: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1297 - accuracy: 0.9654 - val_loss: 0.3457 - val_accuracy: 0.9142\n","Epoch 77/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9661\n","Epoch 77: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1278 - accuracy: 0.9661 - val_loss: 0.3332 - val_accuracy: 0.9148\n","Epoch 78/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9662\n","Epoch 78: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 266ms/step - loss: 0.1262 - accuracy: 0.9662 - val_loss: 0.3272 - val_accuracy: 0.9155\n","Epoch 79/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9669\n","Epoch 79: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 265ms/step - loss: 0.1245 - accuracy: 0.9669 - val_loss: 0.3139 - val_accuracy: 0.9172\n","Epoch 80/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9667\n","Epoch 80: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 290ms/step - loss: 0.1232 - accuracy: 0.9667 - val_loss: 0.3140 - val_accuracy: 0.9173\n","Epoch 81/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9667\n","Epoch 81: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1229 - accuracy: 0.9667 - val_loss: 0.3103 - val_accuracy: 0.9181\n","Epoch 82/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9666\n","Epoch 82: val_accuracy did not improve from 0.91838\n","4/4 [==============================] - 1s 292ms/step - loss: 0.1219 - accuracy: 0.9666 - val_loss: 0.3078 - val_accuracy: 0.9182\n","Epoch 83/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9666\n","Epoch 83: val_accuracy improved from 0.91838 to 0.91876, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 2s 740ms/step - loss: 0.1208 - accuracy: 0.9666 - val_loss: 0.3031 - val_accuracy: 0.9188\n","Epoch 84/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9661\n","Epoch 84: val_accuracy improved from 0.91876 to 0.92097, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 354ms/step - loss: 0.1216 - accuracy: 0.9661 - val_loss: 0.2908 - val_accuracy: 0.9210\n","Epoch 85/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9665\n","Epoch 85: val_accuracy improved from 0.92097 to 0.92172, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.1197 - accuracy: 0.9665 - val_loss: 0.2841 - val_accuracy: 0.9217\n","Epoch 86/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9668\n","Epoch 86: val_accuracy improved from 0.92172 to 0.92517, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 380ms/step - loss: 0.1188 - accuracy: 0.9668 - val_loss: 0.2698 - val_accuracy: 0.9252\n","Epoch 87/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9672\n","Epoch 87: val_accuracy improved from 0.92517 to 0.92590, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.1169 - accuracy: 0.9672 - val_loss: 0.2682 - val_accuracy: 0.9259\n","Epoch 88/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9677\n","Epoch 88: val_accuracy improved from 0.92590 to 0.92874, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 380ms/step - loss: 0.1154 - accuracy: 0.9677 - val_loss: 0.2575 - val_accuracy: 0.9287\n","Epoch 89/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9675\n","Epoch 89: val_accuracy did not improve from 0.92874\n","4/4 [==============================] - 1s 294ms/step - loss: 0.1146 - accuracy: 0.9675 - val_loss: 0.2650 - val_accuracy: 0.9265\n","Epoch 90/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9676\n","Epoch 90: val_accuracy did not improve from 0.92874\n","4/4 [==============================] - 1s 268ms/step - loss: 0.1143 - accuracy: 0.9676 - val_loss: 0.2569 - val_accuracy: 0.9283\n","Epoch 91/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9670\n","Epoch 91: val_accuracy did not improve from 0.92874\n","4/4 [==============================] - 1s 292ms/step - loss: 0.1137 - accuracy: 0.9670 - val_loss: 0.2578 - val_accuracy: 0.9275\n","Epoch 92/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9682\n","Epoch 92: val_accuracy improved from 0.92874 to 0.93062, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.1117 - accuracy: 0.9682 - val_loss: 0.2388 - val_accuracy: 0.9306\n","Epoch 93/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9680\n","Epoch 93: val_accuracy improved from 0.93062 to 0.93065, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.1104 - accuracy: 0.9680 - val_loss: 0.2351 - val_accuracy: 0.9307\n","Epoch 94/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9683\n","Epoch 94: val_accuracy improved from 0.93065 to 0.93472, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 357ms/step - loss: 0.1104 - accuracy: 0.9683 - val_loss: 0.2265 - val_accuracy: 0.9347\n","Epoch 95/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9682\n","Epoch 95: val_accuracy improved from 0.93472 to 0.93554, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 381ms/step - loss: 0.1089 - accuracy: 0.9682 - val_loss: 0.2211 - val_accuracy: 0.9355\n","Epoch 96/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9687\n","Epoch 96: val_accuracy improved from 0.93554 to 0.93635, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 355ms/step - loss: 0.1076 - accuracy: 0.9687 - val_loss: 0.2158 - val_accuracy: 0.9363\n","Epoch 97/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9687\n","Epoch 97: val_accuracy improved from 0.93635 to 0.93896, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 381ms/step - loss: 0.1073 - accuracy: 0.9687 - val_loss: 0.2041 - val_accuracy: 0.9390\n","Epoch 98/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9690\n","Epoch 98: val_accuracy did not improve from 0.93896\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1058 - accuracy: 0.9690 - val_loss: 0.2035 - val_accuracy: 0.9389\n","Epoch 99/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9695\n","Epoch 99: val_accuracy improved from 0.93896 to 0.94216, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.1044 - accuracy: 0.9695 - val_loss: 0.1942 - val_accuracy: 0.9422\n","Epoch 100/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9695\n","Epoch 100: val_accuracy did not improve from 0.94216\n","4/4 [==============================] - 1s 269ms/step - loss: 0.1034 - accuracy: 0.9695 - val_loss: 0.1967 - val_accuracy: 0.9418\n","Epoch 101/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9695\n","Epoch 101: val_accuracy improved from 0.94216 to 0.94367, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 353ms/step - loss: 0.1029 - accuracy: 0.9695 - val_loss: 0.1919 - val_accuracy: 0.9437\n","Epoch 102/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9697\n","Epoch 102: val_accuracy did not improve from 0.94367\n","4/4 [==============================] - 1s 271ms/step - loss: 0.1020 - accuracy: 0.9697 - val_loss: 0.1958 - val_accuracy: 0.9413\n","Epoch 103/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9699\n","Epoch 103: val_accuracy did not improve from 0.94367\n","4/4 [==============================] - 1s 269ms/step - loss: 0.1011 - accuracy: 0.9699 - val_loss: 0.1884 - val_accuracy: 0.9434\n","Epoch 104/300\n","4/4 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9698\n","Epoch 104: val_accuracy did not improve from 0.94367\n","4/4 [==============================] - 1s 267ms/step - loss: 0.1008 - accuracy: 0.9698 - val_loss: 0.1911 - val_accuracy: 0.9424\n","Epoch 105/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9703\n","Epoch 105: val_accuracy improved from 0.94367 to 0.94397, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 355ms/step - loss: 0.0993 - accuracy: 0.9703 - val_loss: 0.1864 - val_accuracy: 0.9440\n","Epoch 106/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9704\n","Epoch 106: val_accuracy improved from 0.94397 to 0.94705, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.0989 - accuracy: 0.9704 - val_loss: 0.1767 - val_accuracy: 0.9470\n","Epoch 107/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9704\n","Epoch 107: val_accuracy did not improve from 0.94705\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0980 - accuracy: 0.9704 - val_loss: 0.1769 - val_accuracy: 0.9468\n","Epoch 108/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9706\n","Epoch 108: val_accuracy did not improve from 0.94705\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0972 - accuracy: 0.9706 - val_loss: 0.1781 - val_accuracy: 0.9466\n","Epoch 109/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9707\n","Epoch 109: val_accuracy improved from 0.94705 to 0.94729, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 357ms/step - loss: 0.0966 - accuracy: 0.9707 - val_loss: 0.1750 - val_accuracy: 0.9473\n","Epoch 110/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9710\n","Epoch 110: val_accuracy did not improve from 0.94729\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0956 - accuracy: 0.9710 - val_loss: 0.1783 - val_accuracy: 0.9460\n","Epoch 111/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9712\n","Epoch 111: val_accuracy improved from 0.94729 to 0.94988, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 361ms/step - loss: 0.0946 - accuracy: 0.9712 - val_loss: 0.1665 - val_accuracy: 0.9499\n","Epoch 112/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9714\n","Epoch 112: val_accuracy improved from 0.94988 to 0.95064, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 358ms/step - loss: 0.0938 - accuracy: 0.9714 - val_loss: 0.1640 - val_accuracy: 0.9506\n","Epoch 113/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0939 - accuracy: 0.9710\n","Epoch 113: val_accuracy did not improve from 0.95064\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0939 - accuracy: 0.9710 - val_loss: 0.1641 - val_accuracy: 0.9506\n","Epoch 114/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9715\n","Epoch 114: val_accuracy improved from 0.95064 to 0.95352, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 381ms/step - loss: 0.0929 - accuracy: 0.9715 - val_loss: 0.1543 - val_accuracy: 0.9535\n","Epoch 115/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9711\n","Epoch 115: val_accuracy did not improve from 0.95352\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0931 - accuracy: 0.9711 - val_loss: 0.1553 - val_accuracy: 0.9535\n","Epoch 116/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9710\n","Epoch 116: val_accuracy did not improve from 0.95352\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0928 - accuracy: 0.9710 - val_loss: 0.1558 - val_accuracy: 0.9525\n","Epoch 117/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9716\n","Epoch 117: val_accuracy improved from 0.95352 to 0.95934, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 377ms/step - loss: 0.0918 - accuracy: 0.9716 - val_loss: 0.1351 - val_accuracy: 0.9593\n","Epoch 118/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9712\n","Epoch 118: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0916 - accuracy: 0.9712 - val_loss: 0.1436 - val_accuracy: 0.9561\n","Epoch 119/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9719\n","Epoch 119: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0900 - accuracy: 0.9719 - val_loss: 0.1438 - val_accuracy: 0.9572\n","Epoch 120/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9721\n","Epoch 120: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0893 - accuracy: 0.9721 - val_loss: 0.1503 - val_accuracy: 0.9538\n","Epoch 121/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9721\n","Epoch 121: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0892 - accuracy: 0.9721 - val_loss: 0.1383 - val_accuracy: 0.9581\n","Epoch 122/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9717\n","Epoch 122: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0890 - accuracy: 0.9717 - val_loss: 0.1441 - val_accuracy: 0.9556\n","Epoch 123/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9721\n","Epoch 123: val_accuracy did not improve from 0.95934\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0882 - accuracy: 0.9721 - val_loss: 0.1345 - val_accuracy: 0.9586\n","Epoch 124/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9728\n","Epoch 124: val_accuracy improved from 0.95934 to 0.95986, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 379ms/step - loss: 0.0867 - accuracy: 0.9728 - val_loss: 0.1300 - val_accuracy: 0.9599\n","Epoch 125/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9730\n","Epoch 125: val_accuracy did not improve from 0.95986\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0856 - accuracy: 0.9730 - val_loss: 0.1334 - val_accuracy: 0.9591\n","Epoch 126/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9732\n","Epoch 126: val_accuracy did not improve from 0.95986\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0847 - accuracy: 0.9732 - val_loss: 0.1382 - val_accuracy: 0.9572\n","Epoch 127/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9732\n","Epoch 127: val_accuracy did not improve from 0.95986\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0843 - accuracy: 0.9732 - val_loss: 0.1323 - val_accuracy: 0.9590\n","Epoch 128/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9738\n","Epoch 128: val_accuracy did not improve from 0.95986\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0826 - accuracy: 0.9738 - val_loss: 0.1330 - val_accuracy: 0.9591\n","Epoch 129/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9734\n","Epoch 129: val_accuracy did not improve from 0.95986\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0832 - accuracy: 0.9734 - val_loss: 0.1339 - val_accuracy: 0.9580\n","Epoch 130/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9735\n","Epoch 130: val_accuracy improved from 0.95986 to 0.96190, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 383ms/step - loss: 0.0829 - accuracy: 0.9735 - val_loss: 0.1244 - val_accuracy: 0.9619\n","Epoch 131/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9738\n","Epoch 131: val_accuracy did not improve from 0.96190\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0818 - accuracy: 0.9738 - val_loss: 0.1276 - val_accuracy: 0.9604\n","Epoch 132/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9741\n","Epoch 132: val_accuracy improved from 0.96190 to 0.96313, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 349ms/step - loss: 0.0809 - accuracy: 0.9741 - val_loss: 0.1187 - val_accuracy: 0.9631\n","Epoch 133/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9741\n","Epoch 133: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0805 - accuracy: 0.9741 - val_loss: 0.1243 - val_accuracy: 0.9608\n","Epoch 134/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9748\n","Epoch 134: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 289ms/step - loss: 0.0787 - accuracy: 0.9748 - val_loss: 0.1245 - val_accuracy: 0.9613\n","Epoch 135/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9747\n","Epoch 135: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0787 - accuracy: 0.9747 - val_loss: 0.1243 - val_accuracy: 0.9607\n","Epoch 136/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9751\n","Epoch 136: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0774 - accuracy: 0.9751 - val_loss: 0.1244 - val_accuracy: 0.9607\n","Epoch 137/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9752\n","Epoch 137: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0771 - accuracy: 0.9752 - val_loss: 0.1184 - val_accuracy: 0.9626\n","Epoch 138/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9756\n","Epoch 138: val_accuracy did not improve from 0.96313\n","4/4 [==============================] - 1s 291ms/step - loss: 0.0760 - accuracy: 0.9756 - val_loss: 0.1232 - val_accuracy: 0.9614\n","Epoch 139/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9755\n","Epoch 139: val_accuracy improved from 0.96313 to 0.96357, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 2s 747ms/step - loss: 0.0757 - accuracy: 0.9755 - val_loss: 0.1168 - val_accuracy: 0.9636\n","Epoch 140/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9759\n","Epoch 140: val_accuracy did not improve from 0.96357\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0745 - accuracy: 0.9759 - val_loss: 0.1224 - val_accuracy: 0.9617\n","Epoch 141/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9760\n","Epoch 141: val_accuracy did not improve from 0.96357\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0743 - accuracy: 0.9760 - val_loss: 0.1199 - val_accuracy: 0.9626\n","Epoch 142/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9761\n","Epoch 142: val_accuracy did not improve from 0.96357\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0736 - accuracy: 0.9761 - val_loss: 0.1170 - val_accuracy: 0.9630\n","Epoch 143/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9766\n","Epoch 143: val_accuracy did not improve from 0.96357\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0724 - accuracy: 0.9766 - val_loss: 0.1159 - val_accuracy: 0.9635\n","Epoch 144/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9764\n","Epoch 144: val_accuracy improved from 0.96357 to 0.96526, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 356ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.1093 - val_accuracy: 0.9653\n","Epoch 145/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9762\n","Epoch 145: val_accuracy did not improve from 0.96526\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0724 - accuracy: 0.9762 - val_loss: 0.1189 - val_accuracy: 0.9626\n","Epoch 146/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9769\n","Epoch 146: val_accuracy improved from 0.96526 to 0.96553, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 355ms/step - loss: 0.0711 - accuracy: 0.9769 - val_loss: 0.1085 - val_accuracy: 0.9655\n","Epoch 147/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9772\n","Epoch 147: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0700 - accuracy: 0.9772 - val_loss: 0.1105 - val_accuracy: 0.9650\n","Epoch 148/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9773\n","Epoch 148: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0695 - accuracy: 0.9773 - val_loss: 0.1100 - val_accuracy: 0.9647\n","Epoch 149/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9778\n","Epoch 149: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0682 - accuracy: 0.9778 - val_loss: 0.1098 - val_accuracy: 0.9649\n","Epoch 150/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9783\n","Epoch 150: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0669 - accuracy: 0.9783 - val_loss: 0.1131 - val_accuracy: 0.9642\n","Epoch 151/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9774\n","Epoch 151: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0684 - accuracy: 0.9774 - val_loss: 0.1107 - val_accuracy: 0.9645\n","Epoch 152/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9778\n","Epoch 152: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0674 - accuracy: 0.9778 - val_loss: 0.1113 - val_accuracy: 0.9647\n","Epoch 153/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9788\n","Epoch 153: val_accuracy did not improve from 0.96553\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0651 - accuracy: 0.9788 - val_loss: 0.1112 - val_accuracy: 0.9647\n","Epoch 154/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9788\n","Epoch 154: val_accuracy improved from 0.96553 to 0.96611, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 378ms/step - loss: 0.0652 - accuracy: 0.9788 - val_loss: 0.1057 - val_accuracy: 0.9661\n","Epoch 155/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9788\n","Epoch 155: val_accuracy did not improve from 0.96611\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0647 - accuracy: 0.9788 - val_loss: 0.1091 - val_accuracy: 0.9650\n","Epoch 156/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9797\n","Epoch 156: val_accuracy did not improve from 0.96611\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0625 - accuracy: 0.9797 - val_loss: 0.1073 - val_accuracy: 0.9657\n","Epoch 157/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9795\n","Epoch 157: val_accuracy improved from 0.96611 to 0.96653, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 1s 381ms/step - loss: 0.0628 - accuracy: 0.9795 - val_loss: 0.1048 - val_accuracy: 0.9665\n","Epoch 158/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9798\n","Epoch 158: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 297ms/step - loss: 0.0620 - accuracy: 0.9798 - val_loss: 0.1067 - val_accuracy: 0.9656\n","Epoch 159/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9798\n","Epoch 159: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0617 - accuracy: 0.9798 - val_loss: 0.1094 - val_accuracy: 0.9653\n","Epoch 160/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9801\n","Epoch 160: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0611 - accuracy: 0.9801 - val_loss: 0.1065 - val_accuracy: 0.9660\n","Epoch 161/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9805\n","Epoch 161: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0597 - accuracy: 0.9805 - val_loss: 0.1095 - val_accuracy: 0.9656\n","Epoch 162/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9805\n","Epoch 162: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 297ms/step - loss: 0.0595 - accuracy: 0.9805 - val_loss: 0.1042 - val_accuracy: 0.9663\n","Epoch 163/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9801\n","Epoch 163: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0603 - accuracy: 0.9801 - val_loss: 0.1083 - val_accuracy: 0.9655\n","Epoch 164/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9800\n","Epoch 164: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0602 - accuracy: 0.9800 - val_loss: 0.1060 - val_accuracy: 0.9651\n","Epoch 165/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9809\n","Epoch 165: val_accuracy did not improve from 0.96653\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0582 - accuracy: 0.9809 - val_loss: 0.1064 - val_accuracy: 0.9655\n","Epoch 166/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9816\n","Epoch 166: val_accuracy improved from 0.96653 to 0.96679, saving model to /content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5\n","4/4 [==============================] - 3s 798ms/step - loss: 0.0568 - accuracy: 0.9816 - val_loss: 0.1023 - val_accuracy: 0.9668\n","Epoch 167/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9807\n","Epoch 167: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0582 - accuracy: 0.9807 - val_loss: 0.1054 - val_accuracy: 0.9654\n","Epoch 168/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9806\n","Epoch 168: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 265ms/step - loss: 0.0583 - accuracy: 0.9806 - val_loss: 0.1108 - val_accuracy: 0.9646\n","Epoch 169/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9809\n","Epoch 169: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0577 - accuracy: 0.9809 - val_loss: 0.1056 - val_accuracy: 0.9659\n","Epoch 170/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9817\n","Epoch 170: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0556 - accuracy: 0.9817 - val_loss: 0.1030 - val_accuracy: 0.9664\n","Epoch 171/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9814\n","Epoch 171: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0561 - accuracy: 0.9814 - val_loss: 0.1029 - val_accuracy: 0.9664\n","Epoch 172/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9823\n","Epoch 172: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 0.1033 - val_accuracy: 0.9664\n","Epoch 173/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9825\n","Epoch 173: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0533 - accuracy: 0.9825 - val_loss: 0.1031 - val_accuracy: 0.9663\n","Epoch 174/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9827\n","Epoch 174: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0529 - accuracy: 0.9827 - val_loss: 0.1051 - val_accuracy: 0.9661\n","Epoch 175/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9830\n","Epoch 175: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0522 - accuracy: 0.9830 - val_loss: 0.1083 - val_accuracy: 0.9659\n","Epoch 176/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9829\n","Epoch 176: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0522 - accuracy: 0.9829 - val_loss: 0.1042 - val_accuracy: 0.9658\n","Epoch 177/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9834\n","Epoch 177: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0509 - accuracy: 0.9834 - val_loss: 0.1049 - val_accuracy: 0.9658\n","Epoch 178/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9825\n","Epoch 178: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0527 - accuracy: 0.9825 - val_loss: 0.1092 - val_accuracy: 0.9643\n","Epoch 179/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9829\n","Epoch 179: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0517 - accuracy: 0.9829 - val_loss: 0.1050 - val_accuracy: 0.9659\n","Epoch 180/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9827\n","Epoch 180: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0520 - accuracy: 0.9827 - val_loss: 0.1082 - val_accuracy: 0.9645\n","Epoch 181/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9828\n","Epoch 181: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0518 - accuracy: 0.9828 - val_loss: 0.1066 - val_accuracy: 0.9658\n","Epoch 182/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9828\n","Epoch 182: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0516 - accuracy: 0.9828 - val_loss: 0.1051 - val_accuracy: 0.9650\n","Epoch 183/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9830\n","Epoch 183: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 0.1151 - val_accuracy: 0.9623\n","Epoch 184/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9831\n","Epoch 184: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0504 - accuracy: 0.9831 - val_loss: 0.1118 - val_accuracy: 0.9634\n","Epoch 185/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9836\n","Epoch 185: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0493 - accuracy: 0.9836 - val_loss: 0.1085 - val_accuracy: 0.9645\n","Epoch 186/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9840\n","Epoch 186: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0483 - accuracy: 0.9840 - val_loss: 0.1109 - val_accuracy: 0.9640\n","Epoch 187/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9841\n","Epoch 187: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0481 - accuracy: 0.9841 - val_loss: 0.1107 - val_accuracy: 0.9639\n","Epoch 188/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9842\n","Epoch 188: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0479 - accuracy: 0.9842 - val_loss: 0.1129 - val_accuracy: 0.9640\n","Epoch 189/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9842\n","Epoch 189: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0475 - accuracy: 0.9842 - val_loss: 0.1136 - val_accuracy: 0.9635\n","Epoch 190/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9847\n","Epoch 190: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.1145 - val_accuracy: 0.9634\n","Epoch 191/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9850\n","Epoch 191: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0457 - accuracy: 0.9850 - val_loss: 0.1159 - val_accuracy: 0.9635\n","Epoch 192/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9852\n","Epoch 192: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0450 - accuracy: 0.9852 - val_loss: 0.1145 - val_accuracy: 0.9638\n","Epoch 193/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9844\n","Epoch 193: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0468 - accuracy: 0.9844 - val_loss: 0.1124 - val_accuracy: 0.9639\n","Epoch 194/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844\n","Epoch 194: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 0.1156 - val_accuracy: 0.9639\n","Epoch 195/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9844\n","Epoch 195: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0463 - accuracy: 0.9844 - val_loss: 0.1122 - val_accuracy: 0.9640\n","Epoch 196/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9854\n","Epoch 196: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.1124 - val_accuracy: 0.9644\n","Epoch 197/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9854\n","Epoch 197: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0440 - accuracy: 0.9854 - val_loss: 0.1149 - val_accuracy: 0.9641\n","Epoch 198/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9856\n","Epoch 198: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0435 - accuracy: 0.9856 - val_loss: 0.1190 - val_accuracy: 0.9632\n","Epoch 199/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9852\n","Epoch 199: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 0.1166 - val_accuracy: 0.9632\n","Epoch 200/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9861\n","Epoch 200: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 291ms/step - loss: 0.0424 - accuracy: 0.9861 - val_loss: 0.1143 - val_accuracy: 0.9636\n","Epoch 201/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9858\n","Epoch 201: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0427 - accuracy: 0.9858 - val_loss: 0.1180 - val_accuracy: 0.9625\n","Epoch 202/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9860\n","Epoch 202: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0423 - accuracy: 0.9860 - val_loss: 0.1153 - val_accuracy: 0.9634\n","Epoch 203/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9861\n","Epoch 203: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0420 - accuracy: 0.9861 - val_loss: 0.1146 - val_accuracy: 0.9637\n","Epoch 204/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9862\n","Epoch 204: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.1147 - val_accuracy: 0.9638\n","Epoch 205/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9861\n","Epoch 205: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0415 - accuracy: 0.9861 - val_loss: 0.1127 - val_accuracy: 0.9646\n","Epoch 206/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9867\n","Epoch 206: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0403 - accuracy: 0.9867 - val_loss: 0.1137 - val_accuracy: 0.9641\n","Epoch 207/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9867\n","Epoch 207: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0401 - accuracy: 0.9867 - val_loss: 0.1169 - val_accuracy: 0.9637\n","Epoch 208/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9867\n","Epoch 208: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0401 - accuracy: 0.9867 - val_loss: 0.1122 - val_accuracy: 0.9650\n","Epoch 209/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9868\n","Epoch 209: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0397 - accuracy: 0.9868 - val_loss: 0.1150 - val_accuracy: 0.9640\n","Epoch 210/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9867\n","Epoch 210: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0397 - accuracy: 0.9867 - val_loss: 0.1208 - val_accuracy: 0.9623\n","Epoch 211/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9868\n","Epoch 211: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0395 - accuracy: 0.9868 - val_loss: 0.1126 - val_accuracy: 0.9648\n","Epoch 212/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9866\n","Epoch 212: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0399 - accuracy: 0.9866 - val_loss: 0.1146 - val_accuracy: 0.9639\n","Epoch 213/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9867\n","Epoch 213: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0394 - accuracy: 0.9867 - val_loss: 0.1203 - val_accuracy: 0.9629\n","Epoch 214/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9865\n","Epoch 214: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0400 - accuracy: 0.9865 - val_loss: 0.1149 - val_accuracy: 0.9640\n","Epoch 215/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9870\n","Epoch 215: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.1172 - val_accuracy: 0.9635\n","Epoch 216/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9874\n","Epoch 216: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0381 - accuracy: 0.9874 - val_loss: 0.1157 - val_accuracy: 0.9643\n","Epoch 217/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9875\n","Epoch 217: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0375 - accuracy: 0.9875 - val_loss: 0.1240 - val_accuracy: 0.9617\n","Epoch 218/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9879\n","Epoch 218: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0368 - accuracy: 0.9879 - val_loss: 0.1165 - val_accuracy: 0.9642\n","Epoch 219/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9876\n","Epoch 219: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0370 - accuracy: 0.9876 - val_loss: 0.1207 - val_accuracy: 0.9627\n","Epoch 220/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9881\n","Epoch 220: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.1215 - val_accuracy: 0.9634\n","Epoch 221/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9881\n","Epoch 221: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 0.1161 - val_accuracy: 0.9644\n","Epoch 222/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9886\n","Epoch 222: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0350 - accuracy: 0.9886 - val_loss: 0.1198 - val_accuracy: 0.9636\n","Epoch 223/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9882\n","Epoch 223: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0355 - accuracy: 0.9882 - val_loss: 0.1204 - val_accuracy: 0.9634\n","Epoch 224/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9879\n","Epoch 224: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0360 - accuracy: 0.9879 - val_loss: 0.1181 - val_accuracy: 0.9640\n","Epoch 225/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9878\n","Epoch 225: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0365 - accuracy: 0.9878 - val_loss: 0.1182 - val_accuracy: 0.9646\n","Epoch 226/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9879\n","Epoch 226: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0361 - accuracy: 0.9879 - val_loss: 0.1222 - val_accuracy: 0.9633\n","Epoch 227/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9881\n","Epoch 227: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0354 - accuracy: 0.9881 - val_loss: 0.1190 - val_accuracy: 0.9637\n","Epoch 228/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9878\n","Epoch 228: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.1251 - val_accuracy: 0.9630\n","Epoch 229/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9884\n","Epoch 229: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 265ms/step - loss: 0.0348 - accuracy: 0.9884 - val_loss: 0.1180 - val_accuracy: 0.9647\n","Epoch 230/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9878\n","Epoch 230: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.1233 - val_accuracy: 0.9633\n","Epoch 231/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9882\n","Epoch 231: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0352 - accuracy: 0.9882 - val_loss: 0.1239 - val_accuracy: 0.9624\n","Epoch 232/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9886\n","Epoch 232: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0342 - accuracy: 0.9886 - val_loss: 0.1252 - val_accuracy: 0.9624\n","Epoch 233/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9885\n","Epoch 233: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0343 - accuracy: 0.9885 - val_loss: 0.1241 - val_accuracy: 0.9629\n","Epoch 234/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9885\n","Epoch 234: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0340 - accuracy: 0.9885 - val_loss: 0.1249 - val_accuracy: 0.9627\n","Epoch 235/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9874\n","Epoch 235: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0366 - accuracy: 0.9874 - val_loss: 0.1223 - val_accuracy: 0.9633\n","Epoch 236/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9886\n","Epoch 236: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0337 - accuracy: 0.9886 - val_loss: 0.1335 - val_accuracy: 0.9614\n","Epoch 237/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9887\n","Epoch 237: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0336 - accuracy: 0.9887 - val_loss: 0.1236 - val_accuracy: 0.9631\n","Epoch 238/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9893\n","Epoch 238: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.1235 - val_accuracy: 0.9637\n","Epoch 239/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9897\n","Epoch 239: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.1199 - val_accuracy: 0.9649\n","Epoch 240/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9893\n","Epoch 240: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.1230 - val_accuracy: 0.9632\n","Epoch 241/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9898\n","Epoch 241: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0311 - accuracy: 0.9898 - val_loss: 0.1233 - val_accuracy: 0.9645\n","Epoch 242/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9891\n","Epoch 242: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0325 - accuracy: 0.9891 - val_loss: 0.1221 - val_accuracy: 0.9634\n","Epoch 243/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9900\n","Epoch 243: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 0.1236 - val_accuracy: 0.9640\n","Epoch 244/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9901\n","Epoch 244: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 291ms/step - loss: 0.0302 - accuracy: 0.9901 - val_loss: 0.1221 - val_accuracy: 0.9639\n","Epoch 245/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9907\n","Epoch 245: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0291 - accuracy: 0.9907 - val_loss: 0.1244 - val_accuracy: 0.9640\n","Epoch 246/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9905\n","Epoch 246: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0294 - accuracy: 0.9905 - val_loss: 0.1249 - val_accuracy: 0.9640\n","Epoch 247/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9905\n","Epoch 247: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 266ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.1281 - val_accuracy: 0.9633\n","Epoch 248/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9908\n","Epoch 248: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.1313 - val_accuracy: 0.9623\n","Epoch 249/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9907\n","Epoch 249: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 292ms/step - loss: 0.0288 - accuracy: 0.9907 - val_loss: 0.1238 - val_accuracy: 0.9644\n","Epoch 250/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9908\n","Epoch 250: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.1272 - val_accuracy: 0.9634\n","Epoch 251/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9908\n","Epoch 251: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0283 - accuracy: 0.9908 - val_loss: 0.1268 - val_accuracy: 0.9638\n","Epoch 252/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9901\n","Epoch 252: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0297 - accuracy: 0.9901 - val_loss: 0.1295 - val_accuracy: 0.9633\n","Epoch 253/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9905\n","Epoch 253: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0288 - accuracy: 0.9905 - val_loss: 0.1231 - val_accuracy: 0.9644\n","Epoch 254/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9907\n","Epoch 254: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 0.1289 - val_accuracy: 0.9634\n","Epoch 255/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9904\n","Epoch 255: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0288 - accuracy: 0.9904 - val_loss: 0.1239 - val_accuracy: 0.9642\n","Epoch 256/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9908\n","Epoch 256: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.1264 - val_accuracy: 0.9639\n","Epoch 257/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9909\n","Epoch 257: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.1266 - val_accuracy: 0.9643\n","Epoch 258/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9909\n","Epoch 258: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 297ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.1267 - val_accuracy: 0.9636\n","Epoch 259/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9907\n","Epoch 259: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0282 - accuracy: 0.9907 - val_loss: 0.1287 - val_accuracy: 0.9636\n","Epoch 260/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9905\n","Epoch 260: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0281 - accuracy: 0.9905 - val_loss: 0.1378 - val_accuracy: 0.9615\n","Epoch 261/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9905\n","Epoch 261: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 0.1248 - val_accuracy: 0.9645\n","Epoch 262/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9910\n","Epoch 262: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0272 - accuracy: 0.9910 - val_loss: 0.1314 - val_accuracy: 0.9626\n","Epoch 263/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9909\n","Epoch 263: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.1302 - val_accuracy: 0.9634\n","Epoch 264/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912\n","Epoch 264: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 0.1307 - val_accuracy: 0.9635\n","Epoch 265/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9914\n","Epoch 265: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.1256 - val_accuracy: 0.9648\n","Epoch 266/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9912\n","Epoch 266: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0266 - accuracy: 0.9912 - val_loss: 0.1333 - val_accuracy: 0.9631\n","Epoch 267/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9914\n","Epoch 267: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0262 - accuracy: 0.9914 - val_loss: 0.1274 - val_accuracy: 0.9645\n","Epoch 268/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9914\n","Epoch 268: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 0.1284 - val_accuracy: 0.9652\n","Epoch 269/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9917\n","Epoch 269: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 293ms/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.1290 - val_accuracy: 0.9650\n","Epoch 270/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9910\n","Epoch 270: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 296ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.1302 - val_accuracy: 0.9646\n","Epoch 271/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9912\n","Epoch 271: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.1294 - val_accuracy: 0.9645\n","Epoch 272/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9914\n","Epoch 272: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.1266 - val_accuracy: 0.9648\n","Epoch 273/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9909\n","Epoch 273: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.1282 - val_accuracy: 0.9651\n","Epoch 274/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9910\n","Epoch 274: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.1270 - val_accuracy: 0.9644\n","Epoch 275/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9916\n","Epoch 275: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0254 - accuracy: 0.9916 - val_loss: 0.1266 - val_accuracy: 0.9650\n","Epoch 276/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9918\n","Epoch 276: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.1283 - val_accuracy: 0.9646\n","Epoch 277/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9914\n","Epoch 277: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 276ms/step - loss: 0.0256 - accuracy: 0.9914 - val_loss: 0.1309 - val_accuracy: 0.9639\n","Epoch 278/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9919\n","Epoch 278: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.1299 - val_accuracy: 0.9645\n","Epoch 279/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9915\n","Epoch 279: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 0.1343 - val_accuracy: 0.9636\n","Epoch 280/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9919\n","Epoch 280: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0246 - accuracy: 0.9919 - val_loss: 0.1318 - val_accuracy: 0.9644\n","Epoch 281/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9913\n","Epoch 281: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.1371 - val_accuracy: 0.9629\n","Epoch 282/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9915\n","Epoch 282: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 0.1300 - val_accuracy: 0.9648\n","Epoch 283/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9913\n","Epoch 283: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0256 - accuracy: 0.9913 - val_loss: 0.1353 - val_accuracy: 0.9636\n","Epoch 284/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9911\n","Epoch 284: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 273ms/step - loss: 0.0259 - accuracy: 0.9911 - val_loss: 0.1340 - val_accuracy: 0.9643\n","Epoch 285/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9916\n","Epoch 285: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 0.1348 - val_accuracy: 0.9635\n","Epoch 286/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9915\n","Epoch 286: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 0.1333 - val_accuracy: 0.9643\n","Epoch 287/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9922\n","Epoch 287: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 269ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 0.1335 - val_accuracy: 0.9637\n","Epoch 288/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9923\n","Epoch 288: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1336 - val_accuracy: 0.9643\n","Epoch 289/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9922\n","Epoch 289: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.1320 - val_accuracy: 0.9642\n","Epoch 290/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9929\n","Epoch 290: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0222 - accuracy: 0.9929 - val_loss: 0.1319 - val_accuracy: 0.9645\n","Epoch 291/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9930\n","Epoch 291: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 270ms/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 0.1321 - val_accuracy: 0.9645\n","Epoch 292/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9934\n","Epoch 292: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 273ms/step - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.1321 - val_accuracy: 0.9646\n","Epoch 293/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933\n","Epoch 293: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 268ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.1331 - val_accuracy: 0.9645\n","Epoch 294/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933\n","Epoch 294: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 294ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.1331 - val_accuracy: 0.9647\n","Epoch 295/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9934\n","Epoch 295: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 298ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 0.1337 - val_accuracy: 0.9645\n","Epoch 296/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9935\n","Epoch 296: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 271ms/step - loss: 0.0207 - accuracy: 0.9935 - val_loss: 0.1339 - val_accuracy: 0.9644\n","Epoch 297/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9926\n","Epoch 297: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0226 - accuracy: 0.9926 - val_loss: 0.1344 - val_accuracy: 0.9645\n","Epoch 298/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9932\n","Epoch 298: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 267ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.1352 - val_accuracy: 0.9643\n","Epoch 299/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9926\n","Epoch 299: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 295ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 0.1358 - val_accuracy: 0.9644\n","Epoch 300/300\n","4/4 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9920\n","Epoch 300: val_accuracy did not improve from 0.96679\n","4/4 [==============================] - 1s 272ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 0.1348 - val_accuracy: 0.9643\n","dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","model=SA_UNet(input_size=(desired_size,desired_size,3),start_neurons=16,lr=1e-3,keep_prob=0.82,block_size=7)\n","model.summary()\n","weight='/pretrained_weights/SA_UNet.h5'\n","restore= False\n","\n","if restore and os.path.isfile(weight):\n","    model.load_weights(weight)\n","    print('loaded a pretrained model')\n","\n","model_checkpoint = ModelCheckpoint(weight, monitor='val_accuracy', verbose=1, save_best_only=True)\n","\n","\n","history=model.fit(x_train, y_train,\n","                epochs=300, #first  100 with lr=1e-3,,and last 50 with lr=1e-4\n","                batch_size=4,\n","                # validation_split=0.05,\n","\n","                validation_data=(x_validate, y_validate),\n","                shuffle=True,\n","                callbacks= [TensorBoard(log_dir='./autoencoder'), model_checkpoint])\n","\n","print(history.history.keys())\n","\n","# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('SA-UNet Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validate'], loc='lower right')\n","plt.show()\n","\n","\n","model.load_weights('/content/drive/MyDrive/VascularSignaturesPreeclampsia/Codes/SA_UNet.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":2125,"status":"ok","timestamp":1658356966509,"user":{"displayName":"Ye Tian","userId":"01053288204941555603"},"user_tz":240},"id":"ayqfQrT4rRhe","outputId":"186db892-299a-402f-9be4-cca2004a409d"},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fe203736d50>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["pred = model.predict(x_validate)\n","plt.imshow(pred[0][:,:,0], cmap = 'gray')"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"SA-UNet for Fundus Vessel Segmentation.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.13 ('RVGANenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"6f64f4bed071435cf5e3440158606e7fb0dab12cb6211ae7f7ca3354575ca766"}}},"nbformat":4,"nbformat_minor":0}
