{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EFWbwLqrLDJI"
      },
      "outputs": [],
      "source": [
        "import json, copy, cv2\n",
        "import networkx as nx\n",
        "import sys,glob\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import gudhi as gd\n",
        "from gudhi.representations import vector_methods\n",
        "import ripser\n",
        "import persim\n",
        "from TDA_filtrations import level_set_flooding, save_BD, image_to_pointcloud\n",
        "from custom_functions import *\n",
        "import multiprocessing as mp\n",
        "import skimage.measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ko6kPGbhLDJK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n",
            "['13144_left', '12844_right', '11790_left', '16863_right', '10120_right', '12857_right', '11874_left', '13678_right', '18804_right', '17029_left', '10126_left', '1452_right', '15387_right', '1514_right', '10239_left', '11547_left', '19213_left', '11966_left', '13125_right', '11929_left', '11726_right', '15981_right', '1887_right', '19494_right', '15810_right', '17048_left', '12227_left', '17111_right', '14842_right', '18353_right', '11013_left', '14380_right', '10819_right', '1660_right', '11203_right']\n"
          ]
        }
      ],
      "source": [
        "###### To select a dataset to analyze, uncomment all code under the dataset's name\n",
        "\n",
        "### STARE Expert #1\n",
        "dataset = \"KAGGLE\"\n",
        "# nefi_output_folder = \"../Data/Dataset_1/NEFI_graphs/*/\"\n",
        "image_folder = \"/DATA/kavi/SA-UNET/CHASE_Pretrained_VSI/\"\n",
        "write_folder = \"/DATA/kavi/TDA_yt/\"\n",
        "\n",
        "#  nums = np.arange(1, 2412)\n",
        "import os\n",
        "nums=os.listdir(image_folder)\n",
        "repeat_files = os.listdir(write_folder)\n",
        "for i in range(len(nums)):\n",
        "  nums[i] = nums[i].split('.')[0]\n",
        "\n",
        "nums = [_ for _ in nums if _ + '_VR_persistence_0.npy' not in repeat_files]\n",
        "\n",
        "print(len(nums))\n",
        "print(nums)\n",
        "\n",
        "# data_name = \"DS1_\"\n",
        "# file_name = \"_left\"\n",
        "# nefi_outputs = glob.glob(f\"{nefi_output_folder}*.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBXmhDuyLDJM"
      },
      "source": [
        "## VR filtration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiCFQA5RLDJN",
        "outputId": "378ae5ed-70bc-43a4-c987-c9c44327a10b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing VR filtration for KAGGLE\n",
            "0: 13144_left\n",
            "1: 12844_right\n",
            "2: 11790_left\n",
            "3: 16863_right\n",
            "4: 10120_right\n",
            "5: 12857_right\n",
            "6: 11874_left\n",
            "7: 13678_right\n",
            "8: 18804_right\n",
            "9: 17029_left\n",
            "10: 10126_left\n",
            "11: 1452_right\n",
            "12: 15387_right\n",
            "13: 1514_right\n",
            "14: 10239_left\n",
            "15: 11547_left\n",
            "16: 19213_left\n",
            "17: 11966_left\n",
            "18: 13125_right\n",
            "19: 11929_left\n",
            "20: 11726_right\n",
            "21: 15981_right\n",
            "22: 1887_right\n",
            "23: 19494_right\n",
            "24: 15810_right\n",
            "25: 17048_left\n",
            "26: 12227_left\n",
            "27: 17111_right\n",
            "28: 14842_right\n",
            "29: 18353_right\n",
            "30: 11013_left\n",
            "31: 14380_right\n",
            "32: 10819_right\n",
            "33: 1660_right\n",
            "34: 11203_right\n"
          ]
        }
      ],
      "source": [
        "#Define weighting for persistence images for ripser\n",
        "def weight_ramp(x):\n",
        "    \n",
        "    if np.any(np.isinf(x)):\n",
        "        weight = 1.0\n",
        "    else:\n",
        "        weight = x[1]/185\n",
        "    \n",
        "    return weight\n",
        "\n",
        "def VR_filtration(num):\n",
        "    \n",
        "    if \"all\" in dataset:\n",
        "        num_str = num\n",
        "    else:\n",
        "        num_str = f\"{str(num).zfill(4)}\"\n",
        "    \n",
        "    #load in image\n",
        "    # image_loc = f\"{image_folder}{file_name}{num_str}.png\"\n",
        "    image_loc = f\"{image_folder}{num_str}.jpeg\"\n",
        "    image = mpimg.imread(image_loc)\n",
        "    \n",
        "    if dataset == \"HRF\":\n",
        "        #downsample for HRF to ease computation\n",
        "        image = skimage.measure.block_reduce(image,(3,3),np.max)\n",
        "    \n",
        "    #saving\n",
        "    filename_header = write_folder+num_str+\"_VR\"\n",
        "    \n",
        "    #convert image to pointcloud\n",
        "    try:\n",
        "        pointcloud = image_to_pointcloud(image[:,:,0])\n",
        "    except:\n",
        "        pointcloud = image_to_pointcloud(image)    \n",
        "    pointcloud = np.array(pointcloud)\n",
        "\n",
        "\n",
        "    #initialize averaged PIs for \\each descriptor vector\n",
        "    im0_ripser_ramp = np.zeros((2500,))\n",
        "    im1_ripser_ramp = np.zeros((2500,))\n",
        "    im0_ripser_ones = np.zeros((2500,))\n",
        "    im1_ripser_ones = np.zeros((2500,))\n",
        "    \n",
        "    np.random.seed(10)\n",
        "    \n",
        "    #shuffle pointcloud\n",
        "    np.random.shuffle(pointcloud)\n",
        "\n",
        "    #Run VR on subsampled pointcloud\n",
        "    dgms = ripser.ripser(pointcloud, n_perm = 2000)['dgms']\n",
        "    \n",
        "    #Save the persistence diagram\n",
        "    save_BD(dgms, filename = f\"{filename_header}_persistence_{0}\")\n",
        "    \n",
        "\n",
        "print(f\"Computing VR filtration for {dataset}\")\n",
        "# pool = mp.Pool(mp.cpu_count())\n",
        "# results = pool.map(VR_filtration, nums)\n",
        "# pool.close()\n",
        "\n",
        "for i, num in enumerate(nums):\n",
        "    print(f'{i}: {num}')\n",
        "    VR_filtration(num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the Radial inward and Radial outward filtrations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def radial_filtrations(num):\n",
        "    \n",
        "    if \"all\" in dataset:\n",
        "        num_str = num\n",
        "    else:\n",
        "        num_str = f\"{str(num).zfill(4)}\"\n",
        "        \n",
        "    if dataset == \"HRF\":\n",
        "        max_rad = 3000\n",
        "    else:\n",
        "        max_rad = 700\n",
        "    \n",
        "    #find nefi output file\n",
        "    nefi_output = [s for s in nefi_outputs if num_str in s]\n",
        "    #ensure there is only one location in this list\n",
        "    assert len(nefi_output)==1\n",
        "    #read in graph G\n",
        "    graph_in = nx.read_multiline_adjlist(nefi_output[0],delimiter='|')\n",
        "\n",
        "    #compute both radial inward and radial outward filtrations\n",
        "    for direction in ['inward','outward']:\n",
        "    \n",
        "        filename_header = write_folder+data_name+file_name+num_str+\"_\"+direction\n",
        "    \n",
        "        diag = radius_filtration(graph_in,max_rad=max_rad,filename_save = filename_header+\"_persistence\",direction=direction)\n",
        "    \n",
        "        b0,b1,r = betti_curve(diag,r0=0,r1=40,filename_save = filename_header+\"_Betti\")\n",
        "        \n",
        "        PI_o, PI_r = Persist_im(diag=diag, inf_val = 40,sigma = 1.0, filename_save = [filename_header+\"_PIO\",\n",
        "                                                                                      filename_header+\"_PIR\"])\n",
        "\n",
        "print(f\"Computing radial filtrations for {dataset}\")          \n",
        "pool = mp.Pool(mp.cpu_count())\n",
        "results = pool.map(radial_filtrations, nums)\n",
        "pool.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the Flooding filtration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def flood_filtration(num):\n",
        "\n",
        "    if dataset == \"all\":\n",
        "        num_str = num\n",
        "    else:\n",
        "        num_str = num #.split('_')[0]\n",
        "       # print('num',num)\n",
        "       # num_str = f\"{str(num).zfill(4)}\"\n",
        "      #  print('num_str', num_str)\n",
        "    \n",
        "    #load in image\n",
        "    image_loc = f\"{image_folder}{num}.jpeg\"\n",
        "    # print('file name', file_name)\n",
        "    # print('image loc', image_loc)\n",
        "    image = mpimg.imread(image_loc)\n",
        "    print(image.shape)\n",
        "    \n",
        "    if dataset == \"HRF\":\n",
        "        #downsample to ease computation\n",
        "        image = skimage.measure.block_reduce(image,(3,3),np.max)\n",
        "    \n",
        "\n",
        "    filename_header = write_folder+data_name+num_str+\"_flooding\"\n",
        "    print('111')\n",
        "    \n",
        "    try:\n",
        "       # print(image[448:672,448:672,0].shape)\n",
        "        diag = level_set_flooding(image[:,:, 0],iter_num=35,steps=2,filename = filename_header+\"_persistence\")\n",
        "\n",
        "    except:\n",
        "        diag = level_set_flooding(image, iter_num=35,steps=2,filename = filename_header+\"_persistence\")\n",
        "\n",
        "   # b0,b1,r = betti_curve(diag,r0=0,r1=35,filename_save = filename_header+\"_Betti\")\n",
        "\n",
        "    \n",
        "   PI_o, PI_r = Persist_im(diag=diag, sigma = 1.0, inf_val = 35,filename_save = [filename_header+\"_PIO\",filename_header+\"_PIR\"])\n",
        "   print(type(PI_o))\n",
        "\n",
        "    print('working on:' ,filename_header)\n",
        "\n",
        "print(f\"Computing flooding filtration for {dataset}\")      \n",
        "# pool = mp.Pool(mp.cpu_count())\n",
        "# results = pool.map(flood_filtration, nums)\n",
        "# pool.close()\n",
        "for i in range(len(nums)):\n",
        "  flood_filtration(nums[i])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('gw2447')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f65f984b75a46b584275f04d72b8a2910d4231d90e2e36c2f604d3138b6f5352"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
